{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60eb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import itertools\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "from transformers import ViltProcessor, ViltModel, ViltConfig\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "        datefmt='%m/%d/%Y %H:%M:%S',\n",
    "        level=logging.INFO)\n",
    "transformers_logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc86ff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cl_evaluation.evaluate_cl_algorithm import forward_transfer_eval, catastrophic_forgetting_eval\n",
    "from configs.model_configs import model_configs\n",
    "from configs.task_configs import task_configs, SUPPORTED_VL_TASKS\n",
    "from utils.seed_utils import set_seed\n",
    "\n",
    "class Args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_workers = 0\n",
    "        self.batch_size = 32\n",
    "        self.mcl_data_dir = '/data/datasets/MCL/'\n",
    "        self.pretrained_model_name = 'dandelin/vilt-b32-mlm'\n",
    "        self.adapter_config = 'houlsby'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "vilt_processor = ViltProcessor.from_pretrained(args.pretrained_model_name)\n",
    "vilt = ViltModel.from_pretrained(args.pretrained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b27f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from modeling.vilt_modeling import ViltContinualLearner, ViltEncoderWrapper\n",
    "\n",
    "ordered_cl_tasks = ['vqa', 'nlvr2', 'snli-ve']\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "model_config = model_configs['vilt']\n",
    "encoder_dim = model_config['encoder_dim']\n",
    "visual_mode = model_config['visual_mode']\n",
    "batch2inputs_converter = model_config['batch2inputs_converter']\n",
    "\n",
    "encoder = ViltEncoderWrapper(vilt_processor, vilt, device)\n",
    "model = ViltContinualLearner(ordered_cl_tasks, encoder, encoder_dim, task_configs)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adf72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = os.path.join('/data/experiments/MCL', 'vilt-sequential_ft-task0_vqa-task1_nlvr2-task2_snli-ve', \\\n",
    "                        'checkpoints', 'task2_snli-ve', 'model')\n",
    "model.load_state_dict(torch.load(ckpt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e685f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers.adapters import AdapterConfig\n",
    "adapter_config = AdapterConfig.load(args.adapter_config)\n",
    "model.add_adapter('snli-ve', config=adapter_config)\n",
    "model.add_adapter('nlvr2', config=adapter_config)\n",
    "model.to(device)\n",
    "model.set_active_adapters('snli-ve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94217ffd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data.visionlanguage_datasets.snli_ve_dataset import build_snli_ve_dataloader\n",
    "from data.image_datasets.flickr30kimages_dataset import Flickr30KImagesDataset\n",
    "\n",
    "    \n",
    "snli_ve_config = task_configs['snli-ve']\n",
    "data_dir = os.path.join(args.mcl_data_dir, snli_ve_config['data_dir'])\n",
    "images_source = snli_ve_config['images_source']\n",
    "flickr30k_config = task_configs[images_source]\n",
    "images_dataset = Flickr30KImagesDataset(os.path.join(args.mcl_data_dir, flickr30k_config['data_dir']))\n",
    "\n",
    "val_dataloader = build_snli_ve_dataloader(args=args,\n",
    "                                          data_dir=data_dir,\n",
    "                                          images_dataset=images_dataset,\n",
    "                                          split='dev',\n",
    "                                          tokenizer=None,\n",
    "                                          visual_mode=visual_mode)\n",
    "model.eval()\n",
    "eval_score = 0\n",
    "t = tqdm(val_dataloader, desc='Evaluating on SNLI-VE val set')\n",
    "total = 0\n",
    "for step, batch in enumerate(t):\n",
    "    inputs = batch2inputs_converter(batch)\n",
    "    if step+1 == 10:\n",
    "        break\n",
    "    with torch.no_grad():\n",
    "        output = model(task_key='snli-ve', **inputs)\n",
    "        logits = output[1]\n",
    "\n",
    "    batch_scores = (logits.argmax(-1).cpu() == batch['labels'])\n",
    "    eval_score += batch_scores.sum().item()\n",
    "    total += batch_scores.shape[0]\n",
    "    t.set_postfix({'score': eval_score/total})\n",
    "eval_score = eval_score/len(val_dataloader.dataset)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377c5dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data.visionlanguage_datasets.nlvr2_dataset import build_nlvr2_dataloader\n",
    "\n",
    "    \n",
    "nlvr_config = task_configs['nlvr2']\n",
    "data_dir = os.path.join(args.mcl_data_dir, nlvr_config['data_dir'])\n",
    "\n",
    "val_dataloader = build_nlvr2_dataloader(args=args,\n",
    "                                          data_dir=data_dir,\n",
    "                                          split='val',\n",
    "                                          visual_mode=visual_mode)\n",
    "model.eval()\n",
    "eval_score = 0\n",
    "t = tqdm(val_dataloader, desc='Evaluating on NLVR2 val set')\n",
    "total = 0\n",
    "for step, batch in enumerate(t):\n",
    "    inputs = batch2inputs_converter(batch)\n",
    "    if step+1 == 10:\n",
    "        break\n",
    "    with torch.no_grad():\n",
    "        output = model(task_key='nlvr2', **inputs)\n",
    "        logits = output[1]\n",
    "\n",
    "    batch_scores = (logits.argmax(-1).cpu() == batch['labels'])\n",
    "    eval_score += batch_scores.sum().item()\n",
    "    total += batch_scores.shape[0]\n",
    "    t.set_postfix({'score': eval_score/total})\n",
    "eval_score = eval_score/len(val_dataloader.dataset)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8deb194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956b345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mclada",
   "language": "python",
   "name": "mclada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
